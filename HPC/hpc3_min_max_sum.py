# -*- coding: utf-8 -*-
"""HPC3_Min_Max_Sum.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NWeIar0l94DoFZsFuyKB2OI0XDON2fiA
"""

!pip install git+https://github.com/afnan47/cuda.git

# Commented out IPython magic to ensure Python compatibility.
# %load_ext nvcc_plugin

# Commented out IPython magic to ensure Python compatibility.
# %%writefile ass3.cu
# 
# #include <iostream>
# 
# __global__ void minKernel(int *arr, int *minval, int n) {
#     int tid = threadIdx.x + blockIdx.x * blockDim.x;
#     if (tid < n) atomicMin(minval, arr[tid]);
# }
# 
# __global__ void maxKernel(int *arr, int *maxval, int n) {
#     int tid = threadIdx.x + blockIdx.x * blockDim.x;
#     if (tid < n) atomicMax(maxval, arr[tid]);
# }
# 
# __global__ void sumKernel(int *arr, int *sum, int n) {
#     int tid = threadIdx.x + blockIdx.x * blockDim.x;
#     if (tid < n) atomicAdd(sum, arr[tid]);
# }
# 
# int minval(int arr[], int n) {
#     int *d_arr, *d_minval;
#     cudaMalloc(&d_arr, n * sizeof(int));
#     cudaMalloc(&d_minval, sizeof(int));
#     cudaMemcpy(d_arr, arr, n * sizeof(int), cudaMemcpyHostToDevice);
#     cudaMemcpy(d_minval, &arr[0], sizeof(int), cudaMemcpyHostToDevice);
# 
#     minKernel<<<(n + 255) / 256, 256>>>(d_arr, d_minval, n);
# 
#     int minval;
#     cudaMemcpy(&minval, d_minval, sizeof(int), cudaMemcpyDeviceToHost);
# 
#     cudaFree(d_arr);
#     cudaFree(d_minval);
# 
#     return minval;
# }
# 
# int maxval(int arr[], int n) {
#     int *d_arr, *d_maxval;
#     cudaMalloc(&d_arr, n * sizeof(int));
#     cudaMalloc(&d_maxval, sizeof(int));
#     cudaMemcpy(d_arr, arr, n * sizeof(int), cudaMemcpyHostToDevice);
#     cudaMemcpy(d_maxval, &arr[0], sizeof(int), cudaMemcpyHostToDevice);
# 
#     maxKernel<<<(n + 255) / 256, 256>>>(d_arr, d_maxval, n);
# 
#     int maxval;
#     cudaMemcpy(&maxval, d_maxval, sizeof(int), cudaMemcpyDeviceToHost);
# 
#     cudaFree(d_arr);
#     cudaFree(d_maxval);
# 
#     return maxval;
# }
# 
# int sum(int arr[], int n) {
#     int *d_arr, *d_sum;
#     cudaMalloc(&d_arr, n * sizeof(int));
#     cudaMalloc(&d_sum, sizeof(int));
#     cudaMemcpy(d_arr, arr, n * sizeof(int), cudaMemcpyHostToDevice);
#     cudaMemcpy(d_sum, &arr[0], sizeof(int), cudaMemcpyHostToDevice);
# 
#     sumKernel<<<(n + 255) / 256, 256>>>(d_arr, d_sum, n);
# 
#     int sum;
#     cudaMemcpy(&sum, d_sum, sizeof(int), cudaMemcpyDeviceToHost);
# 
#     cudaFree(d_arr);
#     cudaFree(d_sum);
# 
#     return sum;
# }
# 
# int average(int arr[], int n) {
#     return (double)sum(arr, n) / n;
# }
# 
# int main() {
#     int n = 5;
#     int arr[] = {1, 2, 3, 4, 5};
# 
#     std::cout << "The minimum value is: " << minval(arr, n) << '\n';
#     std::cout << "The maximum value is: " << maxval(arr, n) << '\n';
#     std::cout << "The summation is: " << sum(arr, n) << '\n';
#     std::cout << "The average is: " << average(arr, n) << '\n';
# 
#     return 0;
# }
#

!nvcc -o ass3 ass3.cu -Xcompiler -fopenmp
!./ass3